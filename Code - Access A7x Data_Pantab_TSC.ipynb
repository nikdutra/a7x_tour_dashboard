{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fed9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                         # Import pandas library for data manipulation and analysis\n",
    "import ast                                                  # Import Abstract Syntax Trees module for parsing Python literals from strings\n",
    "import json                                                 # Import JSON module for encoding/decoding JSON data\n",
    "from geopy.distance import geodesic                         # Import geodesic from geopy.distance to calculate distances between coordinates\n",
    "from spotipy.oauth2 import SpotifyClientCredentials         # Import Spotify credentials manager for API authentication\n",
    "import spotipy                                              # Import Spotipy library for Spotify API interaction\n",
    "import tableauserverclient as TSC                           # Import Tableau Server Client for Tableau integration\n",
    "import pantab as pt                                         # Import Pantab for converting between pandas DataFrames and Tableau Hyper files\n",
    "import requests                                             # Import Requests library for HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a153be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- SETLIST.FM ----------------------------------------# \n",
    "# Replace with your own API key and artist MBID\n",
    "API_KEY = \"your_setlist_api_key\"  # API key for Setlist.fm (replace with your own key)\n",
    "ARTIST_MBID = \"your_setlist_artist_id\"  # MusicBrainz Identifier for the artist\n",
    "\n",
    "# --------------------------------- SPOTIFY ------------------------------------------# \n",
    "# Configure spotipy with your Spotify credentials\n",
    "client_id = 'your_spotify_client_id'  # Your Spotify Client ID\n",
    "client_secret = 'your_spotify_client_secret'  # Your Spotify Client Secret\n",
    "\n",
    "# -------------------------------- TABLEAU API ----------------------------------------# \n",
    "# Tableau Server Personal Access Token credentials and connection details\n",
    "TOKENNAME = 'token_name'  # Name of the Tableau PAT\n",
    "TOKENVALUE = 'token_value'  # Token value (keep secure)\n",
    "CONTENTURL = 'your_content_url'  # Tableau site content URL\n",
    "SERVER = 'your_site'  # Tableau Cloud URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0acc5",
   "metadata": {},
   "source": [
    "PART I â€“ Hitting the Setlist.Fm API to dig up tour data.\n",
    "Heads up: the raw stuff is still messy â€” weâ€™re just getting started ðŸ¤˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b19ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Fetching page 1...\n",
      "âŒ Error: 403 - {\"message\":\"Forbidden\"}\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_setlists(mbid: str, api_key: str, max_pages: int = 100):\n",
    "    \"\"\"\n",
    "    Fetches all available setlists for a given artist from the Setlist.fm API.\n",
    "\n",
    "    Args:\n",
    "        mbid (str): MusicBrainz Identifier for the artist.\n",
    "        api_key (str): Your Setlist.fm API key.\n",
    "        max_pages (int, optional): Maximum number of pages to fetch. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all retrieved setlists, normalized for easy analysis.\n",
    "\n",
    "    Notes:\n",
    "        - The function respects API limits with a short delay between requests.\n",
    "        - If an error occurs (e.g., wrong API key or MBID), it logs the issue and stops.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.setlist.fm/rest/1.0/artist/{mbid}/setlists\"  # Constructs the API endpoint URL using the artist's MusicBrainz ID\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",  # Specifies that we want the response in JSON format\n",
    "        \"x-api-key\": api_key  # Adds the API key to the request headers for authentication\n",
    "    }\n",
    "\n",
    "    all_setlists = []  # Initialize an empty list to store all setlists\n",
    "    page = 1  # Start with the first page of results\n",
    "\n",
    "    while True:  # Begin an infinite loop that will break when conditions are met\n",
    "        print(f\"ðŸ“„ Fetching page {page}...\")  # Print status message with page number\n",
    "        params = {\"p\": page}  # Set up the page parameter for pagination\n",
    "        response = requests.get(base_url, headers=headers, params=params)  # Make HTTP GET request to the API\n",
    "\n",
    "        if response.status_code != 200:  # Check if the request was not successful\n",
    "            print(f\"âŒ Error: {response.status_code} - {response.text}\")  # Print error message with details\n",
    "            break  # Exit the loop if an error occurs\n",
    "\n",
    "        data = response.json()  # Parse the JSON response\n",
    "        setlists = data.get('setlist', [])  # Extract the setlists array, defaulting to empty list if not found\n",
    "\n",
    "        if not setlists:  # Check if no setlists were returned\n",
    "            print(\"âœ… No more setlists found.\")  # Print completion message\n",
    "            break  # Exit the loop if no more setlists are available\n",
    "\n",
    "        all_setlists.extend(setlists)  # Add the fetched setlists to our collection\n",
    "\n",
    "        # Handle pagination info from response\n",
    "        total = int(data.get('total', 0))  # Get total number of items available\n",
    "        items_per_page = int(data.get('itemsPerPage', 20))  # Get number of items per page, default to 20\n",
    "        max_page_count = (total // items_per_page) + (1 if total % items_per_page else 0)  # Calculate the total number of pages\n",
    "\n",
    "        # Stop if last page reached or max_pages limit\n",
    "        if page >= max_page_count or page >= max_pages:  # Check if we've reached the last page or our limit\n",
    "            break  # Exit the loop if either condition is met\n",
    "\n",
    "        page += 1  # Increment the page counter for the next iteration\n",
    "        time.sleep(0.2)  # Add a small delay to respect API rate limits\n",
    "\n",
    "    # Convert list of setlists to pandas DataFrame\n",
    "    return pd.json_normalize(all_setlists)  # Convert the nested JSON data to a flat pandas DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ“Š Fetch setlists and show first few rows\n",
    "df = fetch_all_setlists(ARTIST_MBID, API_KEY)  # Call the function to get all setlists for the specified artist\n",
    "print(df.head())  # Display the first 5 rows of the resulting DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4deb4",
   "metadata": {},
   "source": [
    "Pulled the tracks from a separate setlist dictionary.\n",
    "Digging deeper to match the chaos with real songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f12311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the results\n",
    "resultados = []  # Initialize an empty list to collect extracted song data\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():  # Loop through each row in the DataFrame with its index\n",
    "    id_valor = row['id']  # Extract the ID value from the current row\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert the string representation of a dictionary/list into a Python object\n",
    "        sets_data = None  # Initialize variable to hold the parsed sets data\n",
    "        \n",
    "        # Check if the value is a string and try to parse it\n",
    "        if isinstance(row['sets.set'], str):  # Check if the 'sets.set' value is a string\n",
    "            try:\n",
    "                # First, try parsing as JSON\n",
    "                sets_data = json.loads(row['sets.set'])  # Attempt to parse the string as JSON\n",
    "            except json.JSONDecodeError:  # Handle JSON parsing errors\n",
    "                try:\n",
    "                    # If JSON fails, try using ast.literal_eval as a fallback\n",
    "                    sets_data = ast.literal_eval(row['sets.set'])  # Use Python's abstract syntax tree parser as fallback\n",
    "                except:  # Handle any parsing errors from ast.literal_eval\n",
    "                    # If both parsing attempts fail, print error and skip to next row\n",
    "                    print(f\"Error processing row {index}, ID: {id_valor}\")  # Log the error\n",
    "                    continue  # Skip to the next row in the DataFrame\n",
    "        else:\n",
    "            # If it's already a list or dictionary, assign it directly\n",
    "            sets_data = row['sets.set']  # Use the value directly if it's already a proper Python object\n",
    "        \n",
    "        # Iterate through the main list\n",
    "        for item in sets_data:  # Loop through each set in the setlist\n",
    "            # Check if 'song' key exists and is a list\n",
    "            if 'song' in item and isinstance(item['song'], list):  # Verify that the 'song' field exists and is a list\n",
    "                for song in item['song']:  # Loop through each song in the set\n",
    "                    # If the song has a 'name', add it to the results\n",
    "                    if 'name' in song:  # Check if the song has a name field\n",
    "                        nome = song['name']  # Extract the song name\n",
    "                        resultados.append({'id': id_valor, 'nome': nome})  # Add the song ID and name to results list\n",
    "                    \n",
    "    except Exception as e:  # Catch any other exceptions that might occur\n",
    "        # Catch and report any unexpected errors during processing\n",
    "        print(f\"Error processing row {index}, ID: {id_valor}: {str(e)}\")  # Log detailed error information\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "result_song_id = pd.DataFrame(resultados)  # Convert the collected results into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd499f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_song_id = result_song_id.rename(columns={'nome': 'song'}) #rename the name of the colunm name for song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae7a90",
   "metadata": {},
   "source": [
    "Uploaded a custom \"from-to\" table for songs and albums.\n",
    "Because not every track shows up clean â€” time to match names and unleash the right album "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232ec9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_album_song = pd.read_excel('a7x_albums_songs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26293308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>Sounding the Seventh Trumpet</td>\n",
       "      <td>To End The Rapture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>Sounding the Seventh Trumpet</td>\n",
       "      <td>Turn The Other Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>Sounding the Seventh Trumpet</td>\n",
       "      <td>Darkness Surrounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>Sounding the Seventh Trumpet</td>\n",
       "      <td>The Art Of Subconscious Illusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>Sounding the Seventh Trumpet</td>\n",
       "      <td>We Come Out At Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2023</td>\n",
       "      <td>Life is but a dream</td>\n",
       "      <td>Easier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2023</td>\n",
       "      <td>Life is but a dream</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2023</td>\n",
       "      <td>Life is but a dream</td>\n",
       "      <td>(O)rdinary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2023</td>\n",
       "      <td>Life is but a dream</td>\n",
       "      <td>(D)eath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2023</td>\n",
       "      <td>Life is but a dream</td>\n",
       "      <td>Life Is But a Dream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year                         Album                                Song\n",
       "0   2001  Sounding the Seventh Trumpet                 To End The Rapture \n",
       "1   2001  Sounding the Seventh Trumpet                 Turn The Other Way \n",
       "2   2001  Sounding the Seventh Trumpet               Darkness Surrounding \n",
       "3   2001  Sounding the Seventh Trumpet   The Art Of Subconscious Illusion \n",
       "4   2001  Sounding the Seventh Trumpet               We Come Out At Night \n",
       "..   ...                           ...                                 ...\n",
       "84  2023           Life is but a dream                              Easier\n",
       "85  2023           Life is but a dream                                   G\n",
       "86  2023           Life is but a dream                          (O)rdinary\n",
       "87  2023           Life is but a dream                             (D)eath\n",
       "88  2023           Life is but a dream                 Life Is But a Dream\n",
       "\n",
       "[89 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_album_song #FROM-TO about the Songs and Album -- I collected this information manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185929a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_album_song = df_album_song.rename(columns={'Song': 'song'}) #rename the name of the colunm Song for song\n",
    "df_album_song = df_album_song.rename(columns={'Album': 'album'}) #rename the name of the colunm Album for album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb4f15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'song'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'song'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result_song_id[\u001b[33m'\u001b[39m\u001b[33msong_upper\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mresult_song_id\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msong\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.str.strip().str.upper()  \u001b[38;5;66;03m# Create a new column with the song name stripped of whitespace and converted to uppercase\u001b[39;00m\n\u001b[32m      2\u001b[39m df_album_song[\u001b[33m'\u001b[39m\u001b[33msong_upper\u001b[39m\u001b[33m'\u001b[39m] = df_album_song[\u001b[33m'\u001b[39m\u001b[33msong\u001b[39m\u001b[33m'\u001b[39m].str.strip().str.upper()  \u001b[38;5;66;03m# Create a new column in the album_song dataframe with song names stripped and uppercased\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'song'"
     ]
    }
   ],
   "source": [
    "result_song_id['song_upper'] = result_song_id['song'].str.strip().str.upper()  # Create a new column with the song name stripped of whitespace and converted to uppercase\n",
    "df_album_song['song_upper'] = df_album_song['song'].str.strip().str.upper()  # Create a new column in the album_song dataframe with song names stripped and uppercased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa020eb9",
   "metadata": {},
   "source": [
    "PART II â€“ Hit the Spotify API to fetch track links and embed codes.\n",
    "So you donâ€™t just see the data â€” you will hear it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)  # Set up authentication for Spotify API using client credentials flow\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)  # Create a Spotify API client instance with the authentication manager\n",
    "\n",
    "def obter_link_spotify(nome_musica, artista=\"Avenged Sevenfold\"):\n",
    "    \"\"\"\n",
    "    Function to search for a song on Spotify and return an embed link\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, try a more specific search\n",
    "        query = f'track:\"{nome_musica}\" artist:\"{artista}\"'  # Create a precise query with track and artist filters\n",
    "        result = sp.search(q=query, limit=5, type='track')  # Search Spotify API with the query, limiting to 5 tracks\n",
    "        \n",
    "        # Check if the search returned any results\n",
    "        if result['tracks']['items']:  # Check if the search returned any results\n",
    "            # Look for the closest match\n",
    "            for item in result['tracks']['items']:  # Loop through each track result\n",
    "                if artista.lower() in item['artists'][0]['name'].lower():  # Case-insensitive check if the artist matches\n",
    "                    music_id = item['id']  # Extract the Spotify track ID\n",
    "                    print(f\"Song found: {item['name']} by {item['artists'][0]['name']}\")  # Log the found track\n",
    "                    return f'https://open.spotify.com/embed/track/{music_id}'  # Return the embed URL with the track ID\n",
    "            \n",
    "            # If no exact match found, use the first result\n",
    "            music_id = result['tracks']['items'][0]['id']  # Get the first result's track ID as fallback\n",
    "            print(f\"Closest match: {result['tracks']['items'][0]['name']} by {result['tracks']['items'][0]['artists'][0]['name']}\")  # Log the closest match\n",
    "            return f'https://open.spotify.com/embed/track/{music_id}'  # Return the embed URL with the track ID\n",
    "        \n",
    "        # If no results with specific search, try a broader search\n",
    "        query = f'{nome_musica} {artista}'  # Create a broader query using song name and artist as general search terms\n",
    "        result = sp.search(q=query, limit=10, type='track')  # Search with broader terms and more results (10)\n",
    "        \n",
    "        if result['tracks']['items']:  # Check if the broader search returned any results\n",
    "            # Look for songs by the requested artist\n",
    "            for item in result['tracks']['items']:  # Loop through each track in the broader search results\n",
    "                if artista.lower() in item['artists'][0]['name'].lower():  # Check if any result matches the requested artist\n",
    "                    music_id = item['id']  # Extract the track ID\n",
    "                    print(f\"Song found in broad search: {item['name']} by {item['artists'][0]['name']}\")  # Log the found track\n",
    "                    return f'https://open.spotify.com/embed/track/{music_id}'  # Return the embed URL\n",
    "            \n",
    "            # If specific artist not found, consider it might be a cover or another artist\n",
    "            music_id = result['tracks']['items'][0]['id']  # Use the first result as a possible cover version\n",
    "            print(f\"Possible cover or different artist: {result['tracks']['items'][0]['name']} by {result['tracks']['items'][0]['artists'][0]['name']}\")  # Log the potential cover\n",
    "            return f'https://open.spotify.com/embed/track/{music_id}'  # Return the embed URL\n",
    "        \n",
    "        # If still not found, try searching only by track name (may be a cover by another artist)\n",
    "        query = f'track:\"{nome_musica}\"'  # Create a query with only the track name as last resort\n",
    "        result = sp.search(q=query, limit=15, type='track')  # Search with more results (15) as it's a less specific search\n",
    "        \n",
    "        if result['tracks']['items']:  # Check if the track-only search returned results\n",
    "            # Choose the first available result\n",
    "            music_id = result['tracks']['items'][0]['id']  # Get the track ID from first result\n",
    "            print(f\"Song found (possibly a cover): {result['tracks']['items'][0]['name']} by {result['tracks']['items'][0]['artists'][0]['name']}\")  # Log the found track\n",
    "            return f'https://open.spotify.com/embed/track/{music_id}'  # Return the embed URL\n",
    "        \n",
    "        return 'Song not found'  # Return message if no tracks were found after all search attempts\n",
    "    except Exception as e:  # Catch any exceptions that might occur during the search process\n",
    "        print(f\"Error searching for song '{nome_musica}': {e}\")  # Log the error with the song name\n",
    "        return 'Search error'  # Return error message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c81f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = df_album_song.drop_duplicates(subset=['song'])  # Create a new DataFrame with only unique songs, removing duplicates based on the 'song' column\n",
    "df_songs_spotify = df_songs[['song', 'song_upper']]  # Create a new DataFrame with only the 'song' and 'song_upper' columns from the unique songs DataFrame\n",
    "df_songs_spotify['spotify_link'] = df_songs_spotify['song'].apply(obter_link_spotify)  # Apply the obter_link_spotify function to each song name, storing the returned Spotify embed links in a new column\n",
    "df_songs_spotify['spotify_code'] = df_songs_spotify['spotify_link'].str.extract(r'track/([^/?]+)')  # Extract the Spotify track ID from the embed link using regex and store it in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eaf1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the original album_song DataFrame with the Spotify information using 'song_upper' as the common key, keeping all rows from the left DataFrame (df_album_song) even if they don't have a match\n",
    "df_album_song = pd.merge(df_album_song, df_songs_spotify[['song_upper', 'spotify_link', 'spotify_code']], on='song_upper', how='left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Merge the setlist song data (result_song_id) with album information and Spotify links using 'song_upper' as the joining key; the 'left' join preserves all rows from result_song_id even if they don't have matching album data\n",
    "result_merged_with_albums = pd.merge(result_song_id, df_album_song[['song_upper', 'album', 'year', 'spotify_link', 'spotify_code']], on='song_upper', how='left') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5d5f8",
   "metadata": {},
   "source": [
    "Part III - Clean the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031653af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the columns you want to keep\n",
    "selected_columns = ['id', 'eventDate', 'info', 'url', 'artist.name', 'venue.name', 'venue.city.name', 'venue.city.state', 'venue.city.stateCode', 'venue.city.coords.lat', 'venue.city.coords.long', 'venue.city.country.code', 'venue.city.country.name', 'tour.name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200000de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Dataframe with the selected colunms\n",
    "filtered_df = df[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.rename(columns={'eventDate': 'date'})  # Rename the 'eventDate' column to the simpler 'date'\n",
    "filtered_df = filtered_df.rename(columns={'artist.name': 'artist_name'})  # Replace the nested column name 'artist.name' with the flattened 'artist_name'\n",
    "filtered_df = filtered_df.rename(columns={'venue.name': 'venue'})  # Simplify 'venue.name' to just 'venue'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.name': 'venue_city'})  # Convert the deeply nested 'venue.city.name' to the more accessible 'venue_city'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.state': 'venue_state'})  # Flatten 'venue.city.state' to 'venue_state'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.stateCode': 'venue_state_code'})  # Change 'venue.city.stateCode' to the more consistent 'venue_state_code'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.coords.lat': 'venue_lat'})  # Convert the geographic coordinate 'venue.city.coords.lat' to the simpler 'venue_lat'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.coords.long': 'venue_long'})  # Rename longitude coordinate from 'venue.city.coords.long' to 'venue_long'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.country.code': 'venue_country_code'})  # Simplify the country code column from nested format to flat 'venue_country_code'\n",
    "filtered_df = filtered_df.rename(columns={'venue.city.country.name': 'venue_country_name'})  # Flatten the country name from 'venue.city.country.name' to 'venue_country_name'\n",
    "filtered_df = filtered_df.rename(columns={'tour.name': 'tour_name'})  # Rename 'tour.name' to the more consistent 'tour_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a32798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "filtered_df['data_datetime'] = pd.to_datetime(filtered_df['date'])\n",
    "\n",
    "# Keep only the Date\n",
    "filtered_df['event_date'] = filtered_df['data_datetime'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7081a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the latitude values down by one row to align the next event's latitude with the current row\n",
    "filtered_df['venue_lat_next'] = filtered_df['venue_lat'].shift(+1)\n",
    "\n",
    "# Shift the longitude values down by one row to align the next event's longitude with the current row\n",
    "filtered_df['venue_long_next'] = filtered_df['venue_long'].shift(+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4869e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that may contain missing geographic coordinates\n",
    "columns_to_fill = ['venue_lat_next', 'venue_long_next', 'venue_lat', 'venue_long']\n",
    "\n",
    "# Replace NaN values in these columns with 0 to avoid errors in further calculations (e.g., distance)\n",
    "filtered_df[columns_to_fill] = filtered_df[columns_to_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ad0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of coordinates by pairing latitude and longitude for origins and destinations\n",
    "origins = list(zip(filtered_df['venue_lat'], filtered_df['venue_long']))\n",
    "destinations = list(zip(filtered_df['venue_lat_next'], filtered_df['venue_long_next']))\n",
    "\n",
    "# Calculate geodesic distance in miles between each pair of origin and destination coordinates\n",
    "filtered_df['distance_miles'] = [geodesic(origin, destination).miles for origin, destination in zip(origins, destinations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60179be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in the tour_name column with the event year (converted to string) from data_datetime\n",
    "filtered_df['tour_name'] = filtered_df['tour_name'].fillna(filtered_df['data_datetime'].dt.year.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the filtered_df and result_merged_with_albums DataFrames\n",
    "merged_df = pd.merge(\n",
    "    filtered_df,                        # First DataFrame to merge\n",
    "    result_merged_with_albums,         # Second DataFrame to merge\n",
    "    on='id',                            # Merge based on the 'id' column\n",
    "    how='outer',                        # Perform an outer join to keep all rows from both DataFrames\n",
    "    suffixes=('_filtered', '_albums')  # Add suffixes to columns with the same name to distinguish their origin\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the merged DataFrame to an Excel file named 'setlist_songsplayed_a7x.xlsx' with a sheet named \"setlist\" and no index column\n",
    "merged_df.to_excel('setlist_songsplayed_a7x.xlsx', sheet_name=\"setlist\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a693f",
   "metadata": {},
   "source": [
    "PART IV â€“ Building the Pantab data frame\n",
    "Transforming collected data into Hyper tables for smooth analysis and magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the first Excel file\n",
    "excel_path1 = r\"C:\\Users\\usuario\\Documents\\TC_DEMO_25_VDS-main\\setlist_songsplayed_a7x.xlsx\"\n",
    "\n",
    "# Name of the sheet to read from the Excel file\n",
    "sheet_name1 = \"setlist\"\n",
    "\n",
    "# Read the specified sheet from the Excel file into a DataFrame\n",
    "df1 = pd.read_excel(excel_path1, sheet_name=sheet_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9208d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the Hyper file will be saved\n",
    "hyper_a7x_setlist = r\"C:\\Users\\usuario\\Documents\\TC_DEMO_25_VDS-main\\a7x_setlist.hyper\"\n",
    "\n",
    "# Create a Hyper file from the DataFrame using Pantab and save it to the defined path\n",
    "pt.frame_to_hyper(df1, hyper_a7x_setlist, table = sheet_name1)\n",
    "\n",
    "# Print confirmation message with the location of the created Hyper file\n",
    "print(f\"[Pantab] Hyper file created at: {hyper_a7x_setlist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c1a0e",
   "metadata": {},
   "source": [
    "PART V â€“ Tableau Server Client â€” Publishing to the Cloud\n",
    "Dropping the dashboard into the wild â€” ready for the world to rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72783e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Tableau Server using a Personal Access Token (PAT)\n",
    "tableau_auth = TSC.PersonalAccessTokenAuth(TOKENNAME, TOKENVALUE, site_id=CONTENTURL)\n",
    "\n",
    "# Create a Tableau Server object to interact with the server, using the latest server API version\n",
    "server = TSC.Server(SERVER, use_server_version=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with server.auth.sign_in(tableau_auth):  # Sign in to Tableau Server using the authentication object\n",
    "    # Retrieve all projects available on the Tableau site\n",
    "    all_project_items, pagination_item = server.projects.get()\n",
    "    \n",
    "    # Print a list of project IDs retrieved from the server\n",
    "    print([proj.id for proj in all_project_items])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Connect to Tableau CLoud and publish a data source ----\n",
    "with server.auth.sign_in(tableau_auth):  # Sign in to Tableau Server using the authentication object\n",
    "    \n",
    "    # Specify the target project ID where the datasource will be published\n",
    "    project_id = '5bc07b92-a197-460a-991d-020e906cc259'\n",
    "    \n",
    "    # Define the local path to the Hyper file to be published\n",
    "    file_path_1 = r'C:\\Users\\usuario\\Documents\\TC_DEMO_25_VDS-main\\a7x_setlist.hyper'\n",
    "\n",
    "    # Create a new datasource item associated with the specified project\n",
    "    new_datasource_1 = TSC.DatasourceItem(project_id)\n",
    "\n",
    "    # Publish the Hyper file as a datasource to Tableau Server, overwriting if it already exists\n",
    "    new_datasource_1 = server.datasources.publish(\n",
    "                        new_datasource_1, file_path_1, 'Overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
